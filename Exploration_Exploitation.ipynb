{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "@webio": {
      "lastCommId": null,
      "lastKernelId": null
    },
    "coursera": {
      "course_slug": "fundamentals-of-reinforcement-learning",
      "graded_item_id": "QpLYg",
      "launcher_item_id": "9ldtk"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Exploration-Exploitation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3d7e0bf7bf37a14b2f4cc8896af5d808",
          "grade": false,
          "grade_id": "cell-c9904c1c46f57746",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "DqppUAK44o6t",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Задание 1: Задача бандитов и разведка/эксплуатация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c8f4f6a23a8695f62a4e02738e204550",
          "grade": false,
          "grade_id": "cell-6ef89310dd46c266",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "-i-fgewW4o6x",
        "colab_type": "text"
      },
      "source": [
        "This notebook will:\n",
        "Этот ноутбук:\n",
        "- Поможет вам создать свой первый алгоритм бандита\n",
        "- Поможет вам понять влияние эпсилон на разведку и узнать о компромиссе между разведкой и эксплуатацией.\n",
        "- Introduce you to some of the reinforcement learning software we are going to use for this specialization\n",
        "- Познакомит вас с некоторым программным обеспечением для обучения с подкреплением\n",
        "\n",
        "This class uses RL-Glue to implement most of our experiments. It was originally designed by Adam White, Brian Tanner, and Rich Sutton. This library will give you a solid framework to understand how reinforcement learning experiments work and how to run your own. If it feels a little confusing at first, don't worry - we are going to walk you through it slowly and introduce you to more and more parts as you progress through the specialization.\n",
        "\n",
        "Этот класс использует RL-Glue для проведения большинства наших экспериментов. Первоначально он был разработан Адамом Уайтом, Брайаном Таннером и Ричем Саттоном. Эта библиотека даст вам прочную основу для понимания того, как работают эксперименты с обучением с подкреплением и как проводить свои собственные. Если поначалу она немного сбивает с толку, не стоит волноваться - мы постепенно познакомим вас со все большим и большим количеством ее частей по мере прохождения курса.\n",
        "\n",
        "Мы предполагаем, что вы раньше использовали ноутбук Jupyter. Но если нет, то все довольно просто. Просто нажмите кнопку **run** или *Shift + Enter*, чтобы запустить каждую из ячеек. Места в коде, которые вам нужно заполнить, будут ясно обозначены."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "237637aecdf568d891f923f2da93ed0e",
          "grade": false,
          "grade_id": "cell-2ebeb8b32ec50907",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "e2YIe3Eh4o6z",
        "colab_type": "text"
      },
      "source": [
        "## Раздел 0: Подготовка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fbe36b78ea23f980c0fec58209d0f136",
          "grade": false,
          "grade_id": "cell-b1f350f6be960eea",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "iX__mGWw4o60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Импорт необходимых библиотек\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "from rlglue.rl_glue import RLGlue\n",
        "import main_agent\n",
        "import ten_arm_env\n",
        "import test_env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4e05f19854ba40ff12c551dec5042704",
          "grade": false,
          "grade_id": "cell-e2a306e4cfd3e433",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "0ZO1sXqs4o68",
        "colab_type": "text"
      },
      "source": [
        "В ячейке выше мы импортируем библиотеки, которые нам нужны для этого задания. Мы используем numpy на протяжении всего курса и иногда даем подсказки, какие методы использовать из numpy. Кроме этого, мы в основном используем Vanilla Python и иногда другую библиотеку, такую ​​как matplotlib, для построения графиков.\n",
        "\n",
        "Вы могли заметить, что мы импортируем ten_arm_env. Это __10-рукий стенд__, представленный в [разделе 2.3](http://www.incompleteideas.net/book/RLbook2018.pdf)  учебника. Мы используем его на протяжении всего этого ноутбука для тестирования наших бандитских агентов. У него есть 10 рук - это действия которые может выполнять агент. Выполнение такого действия генерирует стохастическое вознаграждение из распределения Гаусса с единичной дисперсией. Для каждого действия ожидаемая ценность этого действия выбирается случайным образом из нормального распределения перед каждым запуском. Если вы не знакомы с 10-руким стендом, просмотрите его в учебнике, прежде чем продолжить."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6dfc1a07738ba7ef428ad0d6045b194d",
          "grade": false,
          "grade_id": "cell-753cb03c956b611e",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "mbHKyLAL4o69",
        "colab_type": "text"
      },
      "source": [
        "## Раздел 1: Жадный агент"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "26fc8f97320909c8ac7e8c66f5b73fca",
          "grade": false,
          "grade_id": "cell-8e7576e85bbe82fc",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "gqEYEXr34o6-",
        "colab_type": "text"
      },
      "source": [
        "Мы создадим агента, который будет искать действие с самой высокой ожидаемой наградой. Один из способов, которым может действовать агент - всегда выбирать действие с наибольшей ценностьбю на основе текущих оценок агента. Он называется жадным агентом, поскольку жадно выбирает действие, которое, по его мнению, имеет наибольшую ценность. Посмотрим, что происходит в этом случае.\n",
        "\n",
        "Сначала мы реализуем функцию argmax, которая принимает список ценностей действий и возвращает действие с наибольшей ценностью. Почему мы реализуем нашу собственную, а не используем функцию argmax из библиотеки numpy? Функция argmax из numpy возвращает первое вхождение наибольшего значения. Нам не надо, чтобы так проиходило, поскольку так агент будет выбирать конкретное действие в случае наличия равных наград. Вместо этого мы хотим случайным образом выбирать между самыми высокими значениями. Итак, мы собираемся реализовать нашу собственную функцию argmax. Вы можете посмотреть [np.random.choice] (https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html), о случайном выборе из списка значений."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7891047f2d6f4137ad3a82d2a4390c88",
          "grade": false,
          "grade_id": "cell-00a70af9534c45cb",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "k_LXOZX14o7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def argmax(q_values):\n",
        "    \"\"\"\n",
        "    Принимает список q_values ​​и возвращает индекс элемента\n",
        "    с наибольшим значением. Разрывает связи случайным образом.\n",
        "    \n",
        "    возвращает: int - индекс самого высокого значения в q_values\n",
        "    \"\"\"\n",
        "    top_value = float(\"-inf\")\n",
        "    ties = []\n",
        "    \n",
        "    for i in range(len(q_values)):\n",
        "        # Если ценность в q_values ​​больше, чем наивысшая, обновить top_value и сбросить ties в ноль\n",
        "        # если ценность равна top_value, добавить индекс к ties\n",
        "        # вернуть случайно выбранный индекс из ties.\n",
        "        # ВАШ КОД ЗДЕСЬ\n",
        "        raise NotImplementedError()\n",
        "    return np.random.choice(ties)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp7ws6d34o7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Отладочная ячейка\n",
        "# --------------\n",
        "\n",
        "\n",
        "test_array = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
        "assert argmax(test_array) == 8, \"Check your argmax implementation returns the index of the largest value\"\n",
        "\n",
        "# убедитесь, что np.random.choice вызвана корректно\n",
        "np.random.seed(0)\n",
        "test_array = [1, 0, 0, 1]\n",
        "\n",
        "assert argmax(test_array) == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "478af144d962fce153fc39829fe2b80e",
          "grade": true,
          "grade_id": "cell-f227246db2235e96",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false
        },
        "id": "HXutlX9x4o7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -----------\n",
        "# Тестовая ячейка\n",
        "# -----------\n",
        "\n",
        "test_array = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
        "assert argmax(test_array) == 8, \"Check your argmax implementation returns the index of the largest value\"\n",
        "\n",
        "# Установить случайное начальное число, чтобы результаты были детерминированными\n",
        "np.random.seed(0)\n",
        "test_array = [1, 0, 0, 1]\n",
        "\n",
        "counts = [0, 0, 0, 0]\n",
        "for _ in range(100):\n",
        "    a = argmax(test_array)\n",
        "    counts[a] += 1\n",
        "\n",
        "# убедиться, что argmax не всегда выбирает первое вхождение\n",
        "assert counts[0] != 100, \"Make sure your argmax implementation randomly choooses among the largest values.\"\n",
        "\n",
        "# убедиться, что argmax не всегда выбирает последнее вхождение\n",
        "assert counts[3] != 100, \"Make sure your argmax implementation randomly choooses among the largest values.\"\n",
        "\n",
        "# убедиться, что генератор случайных чисел вызван только один раз когда бы  `argmax` ни вызывался\n",
        "expected = [44, 0, 0, 56] # <-- обратите внимание: не однородный из-за случайности\n",
        "assert counts == expected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "60f1a63e2e8eadfa949c0c7e5641c6c3",
          "grade": false,
          "grade_id": "cell-80dca165281ba2f3",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "OkYILHHI4o7M",
        "colab_type": "text"
      },
      "source": [
        "Теперь мы представляем первую часть агента RL-Glue, который вы будете реализовывать. Здесь мы создадим GreedyAgent и реализуем метод agent_step. Этот метод вызывается каждый раз, когда агент делает шаг. Метод должен возвращать действие, выбранное агентом. Этот метод также обеспечивает обновление оценок агента на основе сигналов, которые он получает из среды.\n",
        "\n",
        "Заполните пропуски в коде, чтобы реализовать жадного агента."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "134adde5ff899852967fc0caa5ec8944",
          "grade": false,
          "grade_id": "cell-582d9e7f86d07eb6",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "N4me5BTL4o7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class GreedyAgent(main_agent.Agent):\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"\n",
        "        Один шаг агента. Входные параметры: награда, наблюдения\n",
        "        возвращает действие, которое агент выбирает на этом временном шаге.\n",
        "        \n",
        "        Аргументы:\n",
        "        reward -- float, награда, которую агент получил от окружения после выполнения последнего действия.\n",
        "        observation -- float, наблюдаемое состояние, в котором находится агент. \n",
        "        Не беспокойтесь об этом, так как вы не будете его использовать до будущих уроков\n",
        "        Возвращаемое значение:\n",
        "        current_action -- int, действие, выбранное агентом на текущем временном шаге.\n",
        "        \"\"\"\n",
        "        ### Переменные класса ###\n",
        "        # self.q_values : Массив, содержащий, по мнению агента, все ценности действий (рук).\n",
        "        # self.arm_count : Массив со счетчиком количества опускания каждой руки\n",
        "        # self.last_action : Действие, которое агент совершил на предыдущем временном шаге.\n",
        "        #######################\n",
        "        \n",
        "        # Update Q values Обновление значения Q \n",
        "        # Подсказка: посмотрите алгоритм в разделе \"Инкрементная реализация\" лекции \"Многорукий бандит\"\n",
        "        # увеличить счетчик в self.arm_count для действия с предыдущего шага времени\n",
        "        # обновление размера шага с использованием self.arm_count\n",
        "        # обновление self.q_values для действия с предыдущего шага времени\n",
        "        \n",
        "        # ВАШ КОД ЗДЕСЬ\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "        # текущее действие = ? # Используйте функцию argmax которую вы создали ранее\n",
        "        # ВАШ КОД ЗДЕСЬ\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "        self.last_action = current_action\n",
        "        \n",
        "        return current_action\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs6pwChC4o7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Отладочная ячейка\n",
        "# --------------\n",
        "\n",
        "# создать фейкового агента для тестирования и установить некоторые начальные условия\n",
        "np.random.seed(1)\n",
        "greedy_agent = GreedyAgent()\n",
        "greedy_agent.q_values = [0, 0, 0.5, 0, 0]\n",
        "greedy_agent.arm_count = [0, 1, 0, 0, 0]\n",
        "greedy_agent.last_action = 1\n",
        "\n",
        "action = greedy_agent.agent_step(reward=1, observation=0)\n",
        "\n",
        "# убедиться, что q_values обновилась корректно\n",
        "assert greedy_agent.q_values == [0, 0.5, 0.5, 0, 0]\n",
        "\n",
        "# убедиться, что argmax выбирает равные наибольшие значения случайным образом\n",
        "assert action == 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3434a959aa0f48af5f4250259dd8f556",
          "grade": true,
          "grade_id": "cell-08fc9e17dec07fd5",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false
        },
        "id": "2A8ypLZz4o7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -----------\n",
        "# Тестовая ячейка\n",
        "# -----------\n",
        "\n",
        "# Построение фейкового агента для проверки и установки некоторых начальных условий\n",
        "greedy_agent = GreedyAgent()\n",
        "greedy_agent.q_values = [0, 0, 1.0, 0, 0]\n",
        "greedy_agent.arm_count = [0, 1, 0, 0, 0]\n",
        "greedy_agent.last_action = 1\n",
        "\n",
        "# tагент выполняет шаг\n",
        "action = greedy_agent.agent_step(reward=1, observation=0)\n",
        "\n",
        "# убедиться, что агент выбирает действие жадно\n",
        "assert action == 2\n",
        "\n",
        "# убедиться, что q_values было обновлено корректно\n",
        "assert greedy_agent.q_values == [0, 0.5, 1.0, 0, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "90a42fd6968847f33177fb88b0154707",
          "grade": false,
          "grade_id": "cell-0edf7d5d440cdc40",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "WeCjCD7F4o7W",
        "colab_type": "text"
      },
      "source": [
        "Визуализируем результат. Здесь мы проведем эксперимент с использованием RL-Glue для тестирования нашего агента. Для начала мы настроим код эксперимента; в будущих уроках вы будете делать это самостоятельно."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "bba60ee62523862af65ca4b97dd0fcc6",
          "grade": false,
          "grade_id": "cell-13bf4a5ec5402a22",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "cW-AAiSr4o7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------\n",
        "# Дискуссионная ячейка\n",
        "# ---------------\n",
        "\n",
        "num_runs = 200                    # количество запусков эксперимента\n",
        "num_steps = 1000                  # Сколько раз агентом выбрана каждая рука\n",
        "env = ten_arm_env.Environment     # Устанавливаем, какую среду мы хотим использовать для тестирования\n",
        "agent = GreedyAgent               # Выбираем, какого агента мы хотим использовать\n",
        "agent_info = {\"num_actions\": 10}  # Передаем агенту необходимую информацию. (Здесь - сколько всего рук).\n",
        "env_info = {}                     # Передаем среде необходимую информацию. (В этом случае - ничего не передаем)\n",
        "\n",
        "all_averages = []\n",
        "\n",
        "average_best = 0\n",
        "for run in tqdm(range(num_runs)):           # tqdm - создает индикатор выполнения\n",
        "    np.random.seed(run)\n",
        "    \n",
        "    rl_glue = RLGlue(env, agent)          # Создает новый эксперимент RLGlue с окружением и агентом, которые мы выбрали выше.\n",
        "    rl_glue.rl_init(agent_info, env_info) # передаем RLGlue все, что нужно для инициализации агента и среды.\n",
        "    rl_glue.rl_start()                    # запускаем эксперимент\n",
        "\n",
        "    average_best += np.max(rl_glue.environment.arms)\n",
        "    \n",
        "    scores = [0]\n",
        "    averages = []\n",
        "    \n",
        "    for i in range(num_steps):\n",
        "        reward, _, action, _ = rl_glue.rl_step() # Среда и агент делают шаг и возвращают\n",
        "                                                 # награду и выбранное действие.\n",
        "        scores.append(scores[-1] + reward)\n",
        "        averages.append(scores[-1] / (i + 1))\n",
        "    all_averages.append(averages)\n",
        "\n",
        "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "plt.plot([average_best / num_runs for _ in range(num_steps)], linestyle=\"--\")\n",
        "plt.plot(np.mean(all_averages, axis=0))\n",
        "plt.legend([\"Среда и агент делают шаг и возвращаются\", \"Жадный\"])\n",
        "plt.title(\"Средняя награда жадного агента\")\n",
        "plt.xlabel(\"Шаги\")\n",
        "plt.ylabel(\"средняя награда\")\n",
        "plt.show()\n",
        "greedy_scores = np.mean(all_averages, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6d222d49d1f2106fdc61c071b05133e3",
          "grade": false,
          "grade_id": "cell-5db366a4fb0be46e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Pev8Lb-h4o7Z",
        "colab_type": "text"
      },
      "source": [
        "Как дела у нашего агента? Можно ли сделать лучше?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e097e83bdc2cb91f5f54d2ed0a80c79e",
          "grade": false,
          "grade_id": "cell-ca7a4ae176f250d1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "_1sIcszD4o7a",
        "colab_type": "text"
      },
      "source": [
        "## Раздел 2: Epsilon-жадный агент"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9130c5614d2ba27c32e5fe6173a54af7",
          "grade": false,
          "grade_id": "cell-04a8bd103b7af798",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "8y26U9Y84o7a",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Мы узнали о другом способ работы агента(раздел \"компромисс разведка-эксплуатация\" в лекции \"Многорукий бандит\"), где он не всегда проявляет жадность. Вместо этого иногда требуется исследовательское действие. Он делает это для того, чтобы узнать, какое действие на самом деле лучше всего. Если мы всегда выбираем только то, что, по нашему мнению, является лучшим действием на данный момент, мы можем упустить возможность выбрать действительно лучшее действие, потому что мы не узнавали какие еще бывают действия, или, другими словами, не делали разведку.\n",
        "\n",
        "Реализация epsilon-greedy agent ниже. Мы реализуем алгоритм из раздела \"Инкрементная реализация\". Вы можете использовать свой жадный код сверху и посмотреть в [np.random.random](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.random.html), а так же [np.random.randint](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randint.html), для помощи в случайном выборе действия. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d8598190c401a85561155c94f1b7e24d",
          "grade": false,
          "grade_id": "cell-6862cb5ef5702d22",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "iEsKxjO24o7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class EpsilonGreedyAgent(main_agent.Agent):\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"\n",
        "        Takes one step for the agent. It takes in a reward and observation and \n",
        "        returns the action the agent chooses at that time step.\n",
        "        \n",
        "        Arguments:\n",
        "        reward -- float, the reward the agent recieved from the environment after taking the last action.\n",
        "        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it\n",
        "                              until future lessons\n",
        "        Returns:\n",
        "        current_action -- int, the action chosen by the agent at the current time step.\n",
        "        \"\"\"\n",
        "        \n",
        "        ### Useful Class Variables ###\n",
        "        # self.q_values : An array with what the agent believes each of the values of the arm are.\n",
        "        # self.arm_count : An array with a count of the number of times each arm has been pulled.\n",
        "        # self.last_action : The action that the agent took on the previous time step\n",
        "        # self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1)\n",
        "        #######################\n",
        "        \n",
        "        # Update Q values - this should be the same update as your greedy agent above\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "        # Choose action using epsilon greedy\n",
        "        # Randomly choose a number between 0 and 1 and see if it's less than self.epsilon\n",
        "        # (hint: look at np.random.random()). If it is, set current_action to a random action.\n",
        "        # otherwise choose current_action greedily as you did above.\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "        self.last_action = current_action\n",
        "        \n",
        "        return current_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbkKKtaE4o7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Debugging Cell\n",
        "# --------------\n",
        "# Feel free to make any changes to this cell to debug your code\n",
        "\n",
        "# build a fake agent for testing and set some initial conditions\n",
        "np.random.seed(0)\n",
        "e_greedy_agent = EpsilonGreedyAgent()\n",
        "e_greedy_agent.q_values = [0, 0.0, 0.5, 0, 0]\n",
        "e_greedy_agent.arm_count = [0, 1, 0, 0, 0]\n",
        "e_greedy_agent.num_actions = 5\n",
        "e_greedy_agent.last_action = 1\n",
        "e_greedy_agent.epsilon = 0.5\n",
        "\n",
        "# given this random seed, we should see a greedy action (action 2) here\n",
        "action = e_greedy_agent.agent_step(reward=1, observation=0)\n",
        "\n",
        "# -----------------------------------------------\n",
        "# we'll try to guess a few of the trickier places\n",
        "# -----------------------------------------------\n",
        "\n",
        "# make sure to update for the *last_action* not the current action\n",
        "assert e_greedy_agent.q_values != [0, 0.5, 1.0, 0, 0], \"A\"\n",
        "\n",
        "# make sure the stepsize is based on the *last_action* not the current action\n",
        "assert e_greedy_agent.q_values != [0, 1, 0.5, 0, 0], \"B\"\n",
        "\n",
        "# make sure the agent is using the argmax that breaks ties randomly\n",
        "assert action == 2, \"C\"\n",
        "\n",
        "# -----------------------------------------------\n",
        "\n",
        "# let's see what happens for another action\n",
        "np.random.seed(1)\n",
        "e_greedy_agent = EpsilonGreedyAgent()\n",
        "e_greedy_agent.q_values = [0, 0.5, 0.5, 0, 0]\n",
        "e_greedy_agent.arm_count = [0, 1, 0, 0, 0]\n",
        "e_greedy_agent.num_actions = 5\n",
        "e_greedy_agent.last_action = 1\n",
        "e_greedy_agent.epsilon = 0.5\n",
        "\n",
        "# given this random seed, we should see a random action (action 4) here\n",
        "action = e_greedy_agent.agent_step(reward=1, observation=0)\n",
        "\n",
        "# The agent saw a reward of 1, so should increase the value for *last_action*\n",
        "assert e_greedy_agent.q_values == [0, 0.75, 0.5, 0, 0], \"D\"\n",
        "\n",
        "# the agent should have picked a random action for this particular random seed\n",
        "assert action == 4, \"E\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e90fd67af1fe48141bd6d286262c4fca",
          "grade": true,
          "grade_id": "cell-3099aff70dfd2e61",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false
        },
        "id": "KQ7J0Mmw4o7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "# The contents of the cell will be tested by the autograder.\n",
        "# If they do not pass here, they will not pass there.\n",
        "\n",
        "np.random.seed(0)\n",
        "e_greedy_agent = EpsilonGreedyAgent()\n",
        "e_greedy_agent.q_values = [0, 0, 1.0, 0, 0]\n",
        "e_greedy_agent.arm_count = [0, 1, 0, 0, 0]\n",
        "e_greedy_agent.num_actions = 5\n",
        "e_greedy_agent.last_action = 1\n",
        "e_greedy_agent.epsilon = 0.5\n",
        "action = e_greedy_agent.agent_step(reward=1, observation=0)\n",
        "\n",
        "assert e_greedy_agent.q_values == [0, 0.5, 1.0, 0, 0]\n",
        "\n",
        "# manipulate the random seed so the agent takes a random action\n",
        "np.random.seed(1)\n",
        "action = e_greedy_agent.agent_step(reward=0, observation=0)\n",
        "\n",
        "assert action == 4\n",
        "\n",
        "# check to make sure we update value for action 4\n",
        "action = e_greedy_agent.agent_step(reward=1, observation=0)\n",
        "assert e_greedy_agent.q_values == [0, 0.5, 0.0, 0, 1.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5488f20b68110a856dad3a003f51db32",
          "grade": false,
          "grade_id": "cell-762b0b3997c2300f",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "GpLCpZER4o7j",
        "colab_type": "text"
      },
      "source": [
        "Now that we have our epsilon greedy agent created. Let's compare it against the greedy agent with epsilon of 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f6f2f652f24d56bb35a740579b771138",
          "grade": false,
          "grade_id": "cell-2f6cef9d3ecdace7",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "XW_YHLdH4o7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------\n",
        "# Discussion Cell\n",
        "# ---------------\n",
        "\n",
        "# Plot Epsilon greedy results and greedy results\n",
        "num_runs = 200\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "agent = EpsilonGreedyAgent\n",
        "env = ten_arm_env.Environment\n",
        "agent_info = {\"num_actions\": 10, \"epsilon\": epsilon}\n",
        "env_info = {}\n",
        "all_averages = []\n",
        "\n",
        "for run in tqdm(range(num_runs)):\n",
        "    np.random.seed(run)\n",
        "    \n",
        "    rl_glue = RLGlue(env, agent)\n",
        "    rl_glue.rl_init(agent_info, env_info)\n",
        "    rl_glue.rl_start()\n",
        "\n",
        "    scores = [0]\n",
        "    averages = []\n",
        "    for i in range(num_steps):\n",
        "        reward, _, action, _ = rl_glue.rl_step() # The environment and agent take a step and return\n",
        "                                                 # the reward, and action taken.\n",
        "        scores.append(scores[-1] + reward)\n",
        "        averages.append(scores[-1] / (i + 1))\n",
        "    all_averages.append(averages)\n",
        "\n",
        "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "plt.plot([1.55 for _ in range(num_steps)], linestyle=\"--\")\n",
        "plt.plot(greedy_scores)\n",
        "plt.title(\"Average Reward of Greedy Agent vs. E-Greedy Agent\")\n",
        "plt.plot(np.mean(all_averages, axis=0))\n",
        "plt.legend((\"Best Possible\", \"Greedy\", \"Epsilon: 0.1\"))\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Average reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ed0fa5039cf69237a1caf29b273b2942",
          "grade": false,
          "grade_id": "cell-23cf04f952075345",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "oXdwONND4o7m",
        "colab_type": "text"
      },
      "source": [
        "Notice how much better the epsilon-greedy agent did. Because we occasionally choose a random action we were able to find a better long term policy. By acting greedily before our value estimates are accurate, we risk settling on a suboptimal action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6057919769dc2349ccb835468c218ff2",
          "grade": false,
          "grade_id": "cell-edb9184608392c62",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "RtLycjmM4o7m",
        "colab_type": "text"
      },
      "source": [
        "## Section 2.1 Averaging Multiple Runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7c51be606d9d6554fdd916078b0bda57",
          "grade": false,
          "grade_id": "cell-1b55f263f08b1389",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "l7JyJTBm4o7n",
        "colab_type": "text"
      },
      "source": [
        "Did you notice that we averaged over 2000 runs? Why did we do that?\n",
        "\n",
        "To get some insight, let's look at the results of two individual runs by the same agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9896340e89e8cd11bb0a90ef048e9084",
          "grade": false,
          "grade_id": "cell-69d62e83fc1d91bc",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "kpJ8i-614o7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------\n",
        "# Discussion Cell\n",
        "# ---------------\n",
        "\n",
        "# Plot runs of e-greedy agent\n",
        "agent = EpsilonGreedyAgent\n",
        "env = ten_arm_env.Environment\n",
        "agent_info = {\"num_actions\": 10, \"epsilon\": 0.1}\n",
        "env_info = {}\n",
        "all_averages = []\n",
        "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "num_steps = 1000\n",
        "\n",
        "for run in (0, 1):\n",
        "    np.random.seed(run) # Here we set the seed so that we can compare two different runs\n",
        "    averages = []\n",
        "    rl_glue = RLGlue(env, agent)\n",
        "    rl_glue.rl_init(agent_info, env_info)\n",
        "    rl_glue.rl_start()\n",
        "\n",
        "    scores = [0]\n",
        "    for i in range(num_steps):\n",
        "        reward, state, action, is_terminal = rl_glue.rl_step()\n",
        "        scores.append(scores[-1] + reward)\n",
        "        averages.append(scores[-1] / (i + 1))\n",
        "    \n",
        "    plt.plot(averages)\n",
        "\n",
        "plt.title(\"Comparing two independent runs\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Average reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9c6b5d4ea841a388245eb1fdc732a3ed",
          "grade": false,
          "grade_id": "cell-cbabc6468847faab",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "90sNmrPR4o7q",
        "colab_type": "text"
      },
      "source": [
        "Notice how the two runs were different? But, if this is the exact same algorithm, why does it behave differently in these two runs?\n",
        "\n",
        "The answer is that it is due to randomness in the environment and in the agent. Depending on what action the agent randomly starts with, or when it randomly chooses to explore, it can change the results of the runs. And even if the agent chooses the same action, the reward from the environment is randomly sampled from a Gaussian. The agent could get lucky, and see larger rewards for the best action early on and so settle on the best action faster. Or, it could get unlucky and see smaller rewards for best action early on and so take longer to recognize that it is in fact the best action.\n",
        "\n",
        "To be more concrete, let’s look at how many times an exploratory action is taken, for different seeds. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "27f087a7a6f92b6ba2461d66dfc64779",
          "grade": false,
          "grade_id": "cell-a6e9ef699d799240",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "XwxRBLIf4o7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------\n",
        "# Discussion Cell\n",
        "# ---------------\n",
        "print(\"Random Seed 1\")\n",
        "np.random.seed(1)\n",
        "for _ in range(15):\n",
        "    if np.random.random() < 0.1:\n",
        "        print(\"Exploratory Action\")\n",
        "    \n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "print(\"Random Seed 2\")\n",
        "np.random.seed(2)\n",
        "for _ in range(15):\n",
        "    if np.random.random() < 0.1:\n",
        "        print(\"Exploratory Action\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bc8ff22ac82750f9eb3e0f901d5f4166",
          "grade": false,
          "grade_id": "cell-42f5c9cb11fffbb0",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "M1juL9h_4o7s",
        "colab_type": "text"
      },
      "source": [
        "With the first seed, we take an exploratory action three times out of 15, but with the second, we only take an exploratory action once. This can significantly affect the performance of our agent because the amount of exploration has changed significantly.\n",
        "\n",
        "To compare algorithms, we therefore report performance averaged across many runs. We do this to ensure that we are not simply reporting a result that is due to stochasticity, as explained [in the lectures](https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/PtVBs/sequential-decision-making-with-evaluative-feedback). Rather, we want statistically significant outcomes. We will not use statistical significance tests in this course. Instead, because we have access to simulators for our experiments, we use the simpler strategy of running for a large number of runs and ensuring that the confidence intervals do not overlap. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "65cc408096713cec77d263be0fd90b0d",
          "grade": false,
          "grade_id": "cell-1d4132f4b28f4881",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "YxnSTYtI4o7t",
        "colab_type": "text"
      },
      "source": [
        "## Section 3: Comparing values of epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "81b41ca2616b4d370e19c911cf4ab88e",
          "grade": false,
          "grade_id": "cell-f62fa977aac5da68",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "lQBX34WX4o7t",
        "colab_type": "text"
      },
      "source": [
        "Can we do better than an epsilon of 0.1? Let's try several different values for epsilon and see how they perform. We try different settings of key performance parameters to understand how the agent might perform under different conditions.\n",
        "\n",
        "Below we run an experiment where we sweep over different values for epsilon:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e1e772b65c6e29c699f2fb141c37df73",
          "grade": false,
          "grade_id": "cell-4c9881740ba46656",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "ran4BTme4o7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------\n",
        "# Discussion Cell\n",
        "# ---------------\n",
        "\n",
        "# Experiment code for different e-greedy\n",
        "epsilons = [0.0, 0.01, 0.1, 0.4]\n",
        "\n",
        "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "plt.plot([1.55 for _ in range(num_steps)], linestyle=\"--\")\n",
        "\n",
        "n_q_values = []\n",
        "n_averages = []\n",
        "n_best_actions = []\n",
        "\n",
        "num_runs = 200\n",
        "\n",
        "for epsilon in epsilons:\n",
        "    all_averages = []\n",
        "    for run in tqdm(range(num_runs)):\n",
        "        agent = EpsilonGreedyAgent\n",
        "        agent_info = {\"num_actions\": 10, \"epsilon\": epsilon}\n",
        "        env_info = {\"random_seed\": run}\n",
        "\n",
        "        rl_glue = RLGlue(env, agent)\n",
        "        rl_glue.rl_init(agent_info, env_info)\n",
        "        rl_glue.rl_start()\n",
        "        \n",
        "        best_arm = np.argmax(rl_glue.environment.arms)\n",
        "\n",
        "        scores = [0]\n",
        "        averages = []\n",
        "        best_action_chosen = []\n",
        "        \n",
        "        for i in range(num_steps):\n",
        "            reward, state, action, is_terminal = rl_glue.rl_step()\n",
        "            scores.append(scores[-1] + reward)\n",
        "            averages.append(scores[-1] / (i + 1))\n",
        "            if action == best_arm:\n",
        "                best_action_chosen.append(1)\n",
        "            else:\n",
        "                best_action_chosen.append(0)\n",
        "            if epsilon == 0.1 and run == 0:\n",
        "                n_q_values.append(np.copy(rl_glue.agent.q_values))\n",
        "        if epsilon == 0.1:\n",
        "            n_averages.append(averages)\n",
        "            n_best_actions.append(best_action_chosen)\n",
        "        all_averages.append(averages)\n",
        "        \n",
        "    plt.plot(np.mean(all_averages, axis=0))\n",
        "\n",
        "plt.legend([\"Best Possible\"] + epsilons)\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Average reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "621e4edf3ee0456e562f8f61899fafd8",
          "grade": false,
          "grade_id": "cell-1763c2a2a2863158",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "yhAxApC24o7w",
        "colab_type": "text"
      },
      "source": [
        "Why did 0.1 perform better than 0.01?\n",
        "\n",
        "If exploration helps why did 0.4 perform worse that 0.0 (the greedy agent)?\n",
        "\n",
        "Think about these and how you would answer these questions. They are questions in the practice quiz. If you still have questions about it, retake the practice quiz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4107b76e0b504556e7760f38c7c603b2",
          "grade": false,
          "grade_id": "cell-7f65b4e031a22732",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "8kFxEDcL4o7x",
        "colab_type": "text"
      },
      "source": [
        "## Section 4: The Effect of Step Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "dacfdaab4566f744379cf2b63aa38125",
          "grade": false,
          "grade_id": "cell-a12e885539decec6",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "rV55gTZh4o7x",
        "colab_type": "text"
      },
      "source": [
        "In Section 1 of this assignment, we decayed the step size over time based on action-selection counts. The step-size was 1/N(A), where N(A) is the number of times action A was selected. This is the same as computing a sample average. We could also set the step size to be a constant value, such as 0.1. What would be the effect of doing that? And is it better to use a constant or the sample average method? \n",
        "\n",
        "To investigate this question, let’s start by creating a new agent that has a constant step size. This will be nearly identical to the agent created above. You will use the same code to select the epsilon-greedy action. You will change the update to have a constant step size instead of using the 1/N(A) update."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "706014b629a7e271074f562e910272ae",
          "grade": false,
          "grade_id": "cell-fe26903228ef0c50",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "cMt765R24o7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -----------\n",
        "# Graded Cell\n",
        "# -----------\n",
        "class EpsilonGreedyAgentConstantStepsize(main_agent.Agent):\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"\n",
        "        Takes one step for the agent. It takes in a reward and observation and \n",
        "        returns the action the agent chooses at that time step.\n",
        "        \n",
        "        Arguments:\n",
        "        reward -- float, the reward the agent recieved from the environment after taking the last action.\n",
        "        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it\n",
        "                              until future lessons\n",
        "        Returns:\n",
        "        current_action -- int, the action chosen by the agent at the current time step.\n",
        "        \"\"\"\n",
        "        \n",
        "        ### Useful Class Variables ###\n",
        "        # self.q_values : An array with what the agent believes each of the values of the arm are.\n",
        "        # self.arm_count : An array with a count of the number of times each arm has been pulled.\n",
        "        # self.last_action : An int of the action that the agent took on the previous time step.\n",
        "        # self.step_size : A float which is the current step size for the agent.\n",
        "        # self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1)\n",
        "        #######################\n",
        "        \n",
        "        # Update q_values for action taken at previous time step \n",
        "        # using self.step_size intead of using self.arm_count\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "        # Choose action using epsilon greedy. This is the same as you implemented above.\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "        self.last_action = current_action\n",
        "        \n",
        "        return current_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gypNp8u-4o70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Debugging Cell\n",
        "# --------------\n",
        "# Feel free to make any changes to this cell to debug your code\n",
        "\n",
        "for step_size in [0.01, 0.1, 0.5, 1.0]:\n",
        "    e_greedy_agent = EpsilonGreedyAgentConstantStepsize()\n",
        "    e_greedy_agent.q_values = [0, 0, 1.0, 0, 0]\n",
        "    e_greedy_agent.num_actions = 5\n",
        "    e_greedy_agent.last_action = 1\n",
        "    e_greedy_agent.epsilon = 0.0\n",
        "    e_greedy_agent.step_size = step_size\n",
        "    action = e_greedy_agent.agent_step(1, 0)\n",
        "    assert e_greedy_agent.q_values == [0, step_size, 1.0, 0, 0], \"Check that you are updating q_values correctly using the stepsize.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ea1160cbc12cb030918b93b5b5f8bb80",
          "grade": true,
          "grade_id": "cell-ba6bdf28928e3042",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false
        },
        "id": "fiaMgFT84o72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "# The contents of the cell will be tested by the autograder.\n",
        "# If they do not pass here, they will not pass there.\n",
        "\n",
        "np.random.seed(0)\n",
        "# Check Epsilon Greedy with Different Constant Stepsizes\n",
        "for step_size in [0.01, 0.1, 0.5, 1.0]:\n",
        "    e_greedy_agent = EpsilonGreedyAgentConstantStepsize()\n",
        "    e_greedy_agent.q_values = [0, 0, 1.0, 0, 0]\n",
        "    e_greedy_agent.num_actions = 5\n",
        "    e_greedy_agent.last_action = 1\n",
        "    e_greedy_agent.epsilon = 0.0\n",
        "    e_greedy_agent.step_size = step_size\n",
        "    \n",
        "    action = e_greedy_agent.agent_step(1, 0)\n",
        "    \n",
        "    assert e_greedy_agent.q_values == [0, step_size, 1.0, 0, 0]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8323b6b759f469c5a8fc9107f5ac721e",
          "grade": false,
          "grade_id": "cell-a5d327f4d52578e6",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "iGxya0Kt4o74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------\n",
        "# Discussion Cell\n",
        "# ---------------\n",
        "\n",
        "# Experiment code for different step sizes\n",
        "step_sizes = [0.01, 0.1, 0.5, 1.0, '1/N(A)']\n",
        "\n",
        "epsilon = 0.1\n",
        "num_steps = 1000\n",
        "num_runs = 200\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "\n",
        "q_values = {step_size: [] for step_size in step_sizes}\n",
        "true_values = {step_size: None for step_size in step_sizes}\n",
        "best_actions = {step_size: [] for step_size in step_sizes}\n",
        "\n",
        "for step_size in step_sizes:\n",
        "    all_averages = []\n",
        "    for run in tqdm(range(num_runs)):\n",
        "        np.random.seed(run)\n",
        "        agent = EpsilonGreedyAgentConstantStepsize if step_size != '1/N(A)' else EpsilonGreedyAgent\n",
        "        agent_info = {\"num_actions\": 10, \"epsilon\": epsilon, \"step_size\": step_size, \"initial_value\": 0.0}\n",
        "        env_info = {}\n",
        "\n",
        "        rl_glue = RLGlue(env, agent)\n",
        "        rl_glue.rl_init(agent_info, env_info)\n",
        "        rl_glue.rl_start()\n",
        "        \n",
        "        best_arm = np.argmax(rl_glue.environment.arms)\n",
        "\n",
        "        scores = [0]\n",
        "        averages = []\n",
        "        \n",
        "        if run == 0:\n",
        "            true_values[step_size] = np.copy(rl_glue.environment.arms)\n",
        "            \n",
        "        best_action_chosen = []\n",
        "        for i in range(num_steps):\n",
        "            reward, state, action, is_terminal = rl_glue.rl_step()\n",
        "            scores.append(scores[-1] + reward)\n",
        "            averages.append(scores[-1] / (i + 1))\n",
        "            if action == best_arm:\n",
        "                best_action_chosen.append(1)\n",
        "            else:\n",
        "                best_action_chosen.append(0)\n",
        "            if run == 0:\n",
        "                q_values[step_size].append(np.copy(rl_glue.agent.q_values))\n",
        "        best_actions[step_size].append(best_action_chosen)\n",
        "    ax.plot(np.mean(best_actions[step_size], axis=0))\n",
        "\n",
        "plt.legend(step_sizes)\n",
        "plt.title(\"% Best Arm Pulled\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"% Best Arm Pulled\")\n",
        "vals = ax.get_yticks()\n",
        "ax.set_yticklabels(['{:,.2%}'.format(x) for x in vals])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4490c3113b9b460e79a0f92ae6fb2433",
          "grade": false,
          "grade_id": "cell-6704fdb6f4f612fb",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "_ywBSZii4o76",
        "colab_type": "text"
      },
      "source": [
        "Notice first that we are now plotting the amount of time that the best action is taken rather than the average reward. To better  understand the performance of an agent, it can be useful to measure specific behaviors, beyond just how much reward is accumulated. This measure indicates how close the agent’s behaviour is to optimal.\n",
        "\n",
        "It seems as though 1/N(A) performed better than the others, in that it reaches a solution where it takes the best action most frequently. Now why might this be? Why did a step size of 0.5 start out better but end up performing worse? Why did a step size of 0.01 perform so poorly?\n",
        "\n",
        "Let's dig into this further below. Let’s plot how well each agent tracks the true value, where each agent has a different step size method. You do not have to enter any code here, just follow along."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "57dc5b32de917599bd8d329382d04577",
          "grade": false,
          "grade_id": "cell-49e29a510956e277",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "6Yz-SBcO4o76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lock\n",
        "# ---------------\n",
        "# Discussion Cell\n",
        "# ---------------\n",
        "largest = 0\n",
        "num_steps = 1000\n",
        "for step_size in step_sizes:\n",
        "    plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "    largest = np.argmax(true_values[step_size])\n",
        "    plt.plot([true_values[step_size][largest] for _ in range(num_steps)], linestyle=\"--\")\n",
        "    plt.title(\"Step Size: {}\".format(step_size))\n",
        "    plt.plot(np.array(q_values[step_size])[:, largest])\n",
        "    plt.legend([\"True Expected Value\", \"Estimated Value\"])\n",
        "    plt.xlabel(\"Steps\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f0ecb1029a80a609aa9e18b280572f82",
          "grade": false,
          "grade_id": "cell-a0948edb96aacc70",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "DFv8SgOv4o78",
        "colab_type": "text"
      },
      "source": [
        "These plots help clarify the performance differences between the different step sizes. A step size of 0.01 makes such small updates that the agent’s value estimate of the best action does not get close to the actual value. Step sizes of 0.5 and 1.0 both get close to the true value quickly, but are very susceptible to stochasticity in the rewards. The updates overcorrect too much towards recent rewards, and so oscillate around the true value. This means that on many steps, the action that pulls the best arm may seem worse than it actually is.  A step size of 0.1 updates fairly quickly to the true value, and does not oscillate as widely around the true values as 0.5 and 1.0. This is one of the reasons that 0.1 performs quite well. Finally we see why 1/N(A) performed well. Early on while the step size is still reasonably high it moves quickly to the true expected value, but as it gets pulled more its step size is reduced which makes it less susceptible to the stochasticity of the rewards.\n",
        "\n",
        "Does this mean that 1/N(A) is always the best? When might it not be? One possible setting where it might not be as effective is in non-stationary problems. You learned about non-stationarity in the lessons. Non-stationarity means that the environment may change over time. This could manifest itself as continual change over time of the environment, or a sudden change in the environment.\n",
        "\n",
        "Let's look at how a sudden change in the reward distributions affects a step size like 1/N(A). This time we will run the environment for 2000 steps, and after 1000 steps we will randomly change the expected value of all of the arms. We compare two agents, both using epsilon-greedy with epsilon = 0.1. One uses a constant step size of 0.1, the other a step size of 1/N(A) that reduces over time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e6689cd070662940bf6c0f84a217c6ab",
          "grade": false,
          "grade_id": "cell-55536f4ac923ab96",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "_ku-gQFG4o79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------\n",
        "# Discussion Cell\n",
        "# ---------------\n",
        "epsilon = 0.1\n",
        "num_steps = 2000\n",
        "num_runs = 200\n",
        "step_size = 0.1\n",
        "\n",
        "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "plt.plot([1.55 for _ in range(num_steps)], linestyle=\"--\")\n",
        "\n",
        "for agent in [EpsilonGreedyAgent, EpsilonGreedyAgentConstantStepsize]:\n",
        "    all_averages = []\n",
        "    for run in tqdm(range(num_runs)):\n",
        "        agent_info = {\"num_actions\": 10, \"epsilon\": epsilon, \"step_size\": step_size}\n",
        "        np.random.seed(run)\n",
        "        \n",
        "        rl_glue = RLGlue(env, agent)\n",
        "        rl_glue.rl_init(agent_info, env_info)\n",
        "        rl_glue.rl_start()\n",
        "\n",
        "        scores = [0]\n",
        "        averages = []\n",
        "        \n",
        "        for i in range(num_steps):\n",
        "            reward, state, action, is_terminal = rl_glue.rl_step()\n",
        "            scores.append(scores[-1] + reward)\n",
        "            averages.append(scores[-1] / (i + 1))\n",
        "            if i == 1000:\n",
        "                rl_glue.environment.arms = np.random.randn(10)\n",
        "        all_averages.append(averages)\n",
        "        \n",
        "    plt.plot(np.mean(all_averages, axis=0))\n",
        "plt.legend([\"Best Possible\", \"1/N(A)\", \"0.1\"])\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Average reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "714c6cad23e3d8fe31496ffbe8674620",
          "grade": false,
          "grade_id": "cell-c4a8be88bbcc9b38",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "TWJQdWai4o8A",
        "colab_type": "text"
      },
      "source": [
        "Now the agent with a step size of 1/N(A) performed better at the start but then performed worse when the environment changed! What happened?\n",
        "\n",
        "Think about what the step size would be after 1000 steps. Let's say the best action gets chosen 500 times. That means the step size for that action is 1/500 or 0.002. At each step when we update the value of the action and the value is going to move only 0.002 * the error. That is a very tiny adjustment and it will take a long time for it to get to the true value.\n",
        "\n",
        "The agent with step size 0.1, however, will always update in 1/10th of the direction of the error. This means that on average it will take ten steps for it to update its value to the sample mean.\n",
        "\n",
        "These are the types of tradeoffs we have to think about in reinforcement learning. A larger step size moves us more quickly toward the true value, but can make our estimated values oscillate around the expected value. A step size that reduces over time can converge to close to the expected value, without oscillating. On the other hand, such a decaying stepsize is not able to adapt to changes in the environment. Nonstationarity---and the related concept of partial observability---is a common feature of reinforcement learning problems and when learning online.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ad81a549755ccc9ecbfaee839092688a",
          "grade": false,
          "grade_id": "cell-81f8c56a5fa91ecb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "AZyLcBS_4o8A",
        "colab_type": "text"
      },
      "source": [
        "## Section 5: Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3c335943267e235b3001c228e4bf9ba3",
          "grade": false,
          "grade_id": "cell-3c25a546a3d44e22",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "fkp4wgO94o8B",
        "colab_type": "text"
      },
      "source": [
        "Great work! You have:\n",
        "- Implemented your first agent\n",
        "- Learned about the effect of epsilon, an exploration parameter, on the performance of an agent\n",
        "- Learned about the effect of step size on the performance of the agent\n",
        "- Learned about a good experiment practice of averaging across multiple runs"
      ]
    }
  ]
}